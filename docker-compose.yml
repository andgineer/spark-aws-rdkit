version: "3.0"

services:
  # Spark Driver
  spark-master:
    image: andgineer/spark-aws-rdkit
    environment:
      SPARK_MODE: master
      SPARK_LOCAL_HOSTNAME: spark-master
      SPARK_CONF_DIR: /conf/driver
      SPARK_PUBLIC_DNS: $SPARK_PUBLIC_DNS
      SPARK_DAEMON_JAVA_OPTS: "-Dspark.deploy.recoveryMode=FILESYSTEM -Dspark.deploy.recoveryDirectory=/recovery"
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"
    ports:
      - 8080:8080
      - 7077:7077
      - 4040:4040
      - 6066:6066
    expose:
      - 7001
      - 7002
      - 7003
      - 7004
      - 7005
      - 7077
      - 6066

  # Spark worker.
  spark-worker:
    extends:
      file: docker-compose.worker.yml
      service: worker
    environment:
      SPARK_WORKER_PORT: 8881
      SPARK_WORKER_WEBUI_PORT: 8081
    expose:
      - 8881
    ports:
      - "8081:8081"

  # Spark second worker.
  # We cannot scale with `--scale` and `port: "8081-8200:8081"` because we have to set external port explicitly
  spark-coworker:
    extends:
      file: docker-compose.worker.yml
      service: worker
    environment:
      SPARK_WORKER_PORT: 8882
      SPARK_WORKER_WEBUI_PORT: 8082
    expose:
      - 8882
    ports:
      - "8082:8082"

# Spark Submit node with pyspark environment for RDKit
  submit:
    image: andgineer/spark-aws-rdkit
    volumes:
      - ./src:/src
    environment:
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"
    command: python /src/health_check.py
    depends_on:
      - spark-master
      - spark-worker

